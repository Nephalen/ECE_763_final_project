{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "#load models and other customized component\n",
    "from models.base_model import BaseModel, MagicModel1, MagicModel2\n",
    "from utils.model_util import fit, predict, hyp_random_search\n",
    "from utils.loss_functions import ProbCatCrossEntropyLoss\n",
    "from utils.performance import categorical_accuracy\n",
    "from utils.preprocessing import GlobalContrastNormalization, ZCATransformation, zca_whitening_matrix\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#set gcn transform\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     GlobalContrastNormalization(1, 10, 1e-6)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#set gcn zca transform\n",
    "#calculate ZCA whitening\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     GlobalContrastNormalization(1, 10, 1e-6)])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=250, shuffle=False, num_workers=0)\n",
    "\n",
    "data = np.concatenate([d for d, _ in trainloader], axis=0)\n",
    "zca_mean = torch.from_numpy(np.mean(data, axis=0).flatten().astype(np.float32))\n",
    "zca_matrix = torch.from_numpy(zca_whitening_matrix(data).astype(np.float32))\n",
    "\n",
    "#set transform\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     GlobalContrastNormalization(1, 10, 1e-6),\n",
    "     ZCATransformation(zca_matrix, zca_mean)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#set normalization transform\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#set norm zca transform\n",
    "#calculate ZCA whitening\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=250, shuffle=False, num_workers=0)\n",
    "\n",
    "data = np.concatenate([d for d, _ in trainloader], axis=0)\n",
    "zca_mean = torch.from_numpy(np.mean(data, axis=0).flatten().astype(np.float32))\n",
    "zca_matrix = torch.from_numpy(zca_whitening_matrix(data).astype(np.float32))\n",
    "\n",
    "#set transform\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     ZCATransformation(zca_matrix, zca_mean)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#get data (_, 3, 32, 32)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split validation set from training set\n",
    "smallset = Subset(trainset, np.arange(200))\n",
    "\n",
    "train_length = len(trainset)\n",
    "train_index = np.arange(int(train_length*0.8))\n",
    "valid_index = np.arange(int(train_length*0.8), train_length)\n",
    "\n",
    "validset = Subset(trainset, valid_index)\n",
    "trainset = Subset(trainset, train_index)\n",
    "\n",
    "smallloader = DataLoader(smallset, batch_size = 4, shuffle=False, num_workers=4, pin_memory =True)\n",
    "trainloader = DataLoader(trainset, batch_size=250, shuffle=True, num_workers=4, pin_memory =True)\n",
    "validloader = DataLoader(validset, batch_size=250, shuffle=False, num_workers=4, pin_memory =True)\n",
    "testloader = DataLoader(testset, batch_size=250, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model, optimizer, etc\n",
    "model = MagicModel1((3, 32, 32), 10)\n",
    "loss_function = ProbCatCrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3) #, momentum=0.9\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 10, gamma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyp_search_param = {'lr_low': -6, 'lr_high': -3, \n",
    "                    'reg_low': -5, 'reg_high': 0,\n",
    "                    'lr_step_low': 1, 'lr_step_high': 50,\n",
    "                    'lr_gamma_low': -2, 'lr_gamma_high': 0}\n",
    "result = hyp_random_search(MagicModel1, trainloader, validloader, loss_function, epochs = 50, max_count = 60, param=hyp_search_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#save hyperparameter search result\n",
    "with open('hyper1.pickle', 'wb') as handle:\n",
    "    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1% 1/160 [00:01<03:46,  1.42s/it, training_loss=2.29, train_accuracy=0.104]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwang75/tmp/workspace/models/base_model.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100% 160/160 [00:50<00:00,  3.17it/s, training_loss=1.6, train_accuracy=0.426, val_loss=1.29, val_accuracy=0.543]\n",
      "Epoch 1: 100% 160/160 [00:52<00:00,  3.03it/s, training_loss=1.07, train_accuracy=0.632, val_loss=1.26, val_accuracy=0.56]\n",
      "Epoch 2: 100% 160/160 [00:47<00:00,  3.36it/s, training_loss=0.817, train_accuracy=0.725, val_loss=1.09, val_accuracy=0.623]\n",
      "Epoch 3: 100% 160/160 [00:50<00:00,  3.17it/s, training_loss=0.567, train_accuracy=0.821, val_loss=0.995, val_accuracy=0.663]\n",
      "Epoch 4: 100% 160/160 [00:47<00:00,  3.36it/s, training_loss=0.319, train_accuracy=0.913, val_loss=1.43, val_accuracy=0.573]\n",
      "Epoch 5: 100% 160/160 [00:52<00:00,  3.07it/s, training_loss=0.156, train_accuracy=0.966, val_loss=1.46, val_accuracy=0.589]\n",
      "Epoch 6: 100% 160/160 [00:50<00:00,  3.15it/s, training_loss=0.0663, train_accuracy=0.991, val_loss=1.38, val_accuracy=0.626]\n",
      "Epoch 7: 100% 160/160 [00:51<00:00,  3.13it/s, training_loss=0.0346, train_accuracy=0.997, val_loss=1.35, val_accuracy=0.643]\n",
      "Epoch 8: 100% 160/160 [00:48<00:00,  3.28it/s, training_loss=0.0157, train_accuracy=1, val_loss=1.34, val_accuracy=0.647]\n",
      "Epoch 9: 100% 160/160 [00:50<00:00,  3.19it/s, training_loss=0.0106, train_accuracy=1, val_loss=1.4, val_accuracy=0.635]\n",
      "Epoch 10: 100% 160/160 [00:48<00:00,  3.28it/s, training_loss=0.00847, train_accuracy=1, val_loss=1.37, val_accuracy=0.642]\n",
      "Epoch 11: 100% 160/160 [00:47<00:00,  3.36it/s, training_loss=0.00667, train_accuracy=1, val_loss=1.42, val_accuracy=0.642]\n",
      "Epoch 12: 100% 160/160 [00:50<00:00,  3.18it/s, training_loss=0.00619, train_accuracy=1, val_loss=1.42, val_accuracy=0.64]\n",
      "Epoch 13: 100% 160/160 [00:50<00:00,  3.18it/s, training_loss=0.00589, train_accuracy=1, val_loss=1.46, val_accuracy=0.633]\n",
      "Epoch 14: 100% 160/160 [00:49<00:00,  3.22it/s, training_loss=0.00559, train_accuracy=1, val_loss=1.52, val_accuracy=0.617]\n",
      "Epoch 15: 100% 160/160 [00:47<00:00,  3.33it/s, training_loss=0.122, train_accuracy=0.961, val_loss=1.6, val_accuracy=0.559]\n",
      "Epoch 16: 100% 160/160 [00:47<00:00,  3.37it/s, training_loss=0.162, train_accuracy=0.95, val_loss=1.79, val_accuracy=0.579]\n",
      "Epoch 17: 100% 160/160 [00:49<00:00,  3.27it/s, training_loss=0.0647, train_accuracy=0.985, val_loss=1.87, val_accuracy=0.566]\n",
      "Epoch 18: 100% 160/160 [00:45<00:00,  3.53it/s, training_loss=0.0194, train_accuracy=0.997, val_loss=1.86, val_accuracy=0.587]\n",
      "Epoch 19: 100% 160/160 [00:50<00:00,  3.19it/s, training_loss=0.00712, train_accuracy=1, val_loss=1.55, val_accuracy=0.629]\n",
      "Epoch 20: 100% 160/160 [00:46<00:00,  3.43it/s, training_loss=0.00445, train_accuracy=1, val_loss=1.52, val_accuracy=0.635]\n",
      "Epoch 21: 100% 160/160 [00:48<00:00,  3.32it/s, training_loss=0.00397, train_accuracy=1, val_loss=1.51, val_accuracy=0.64]\n",
      "Epoch 22: 100% 160/160 [00:47<00:00,  3.37it/s, training_loss=0.00408, train_accuracy=1, val_loss=1.48, val_accuracy=0.64]\n",
      "Epoch 23: 100% 160/160 [00:51<00:00,  3.08it/s, training_loss=0.00414, train_accuracy=1, val_loss=1.49, val_accuracy=0.635]\n",
      "Epoch 24: 100% 160/160 [00:50<00:00,  3.19it/s, training_loss=0.00425, train_accuracy=1, val_loss=1.49, val_accuracy=0.635]\n",
      "Epoch 25: 100% 160/160 [00:51<00:00,  3.12it/s, training_loss=0.00433, train_accuracy=1, val_loss=1.5, val_accuracy=0.633]\n",
      "Epoch 26: 100% 160/160 [00:47<00:00,  3.37it/s, training_loss=0.00425, train_accuracy=1, val_loss=1.5, val_accuracy=0.628]\n",
      "Epoch 27: 100% 160/160 [00:50<00:00,  3.20it/s, training_loss=0.00436, train_accuracy=1, val_loss=1.5, val_accuracy=0.628]\n",
      "Epoch 28: 100% 160/160 [00:49<00:00,  3.20it/s, training_loss=0.00437, train_accuracy=1, val_loss=1.55, val_accuracy=0.623]\n",
      "Epoch 29: 100% 160/160 [00:50<00:00,  3.20it/s, training_loss=0.00424, train_accuracy=1, val_loss=1.53, val_accuracy=0.623]\n",
      "Epoch 30: 100% 160/160 [00:51<00:00,  3.09it/s, training_loss=0.00418, train_accuracy=1, val_loss=1.59, val_accuracy=0.613]\n",
      "Epoch 31: 100% 160/160 [00:50<00:00,  3.16it/s, training_loss=0.00409, train_accuracy=1, val_loss=1.62, val_accuracy=0.614]\n",
      "Epoch 32: 100% 160/160 [00:50<00:00,  3.15it/s, training_loss=0.00401, train_accuracy=1, val_loss=1.71, val_accuracy=0.591]\n",
      "Epoch 33: 100% 160/160 [00:51<00:00,  3.13it/s, training_loss=0.235, train_accuracy=0.924, val_loss=2.14, val_accuracy=0.518]\n",
      "Epoch 34: 100% 160/160 [00:50<00:00,  3.15it/s, training_loss=0.0797, train_accuracy=0.978, val_loss=1.94, val_accuracy=0.575]\n",
      "Epoch 35: 100% 160/160 [00:50<00:00,  3.16it/s, training_loss=0.0223, train_accuracy=0.996, val_loss=1.8, val_accuracy=0.601]\n",
      "Epoch 36: 100% 160/160 [00:48<00:00,  3.31it/s, training_loss=0.00473, train_accuracy=1, val_loss=1.68, val_accuracy=0.622]\n",
      "Epoch 37: 100% 160/160 [00:49<00:00,  3.23it/s, training_loss=0.00304, train_accuracy=1, val_loss=1.64, val_accuracy=0.622]\n",
      "Epoch 38: 100% 160/160 [00:49<00:00,  3.25it/s, training_loss=0.00299, train_accuracy=1, val_loss=1.64, val_accuracy=0.619]\n",
      "Epoch 39: 100% 160/160 [00:47<00:00,  3.39it/s, training_loss=0.00308, train_accuracy=1, val_loss=1.63, val_accuracy=0.62]\n",
      "Epoch 40: 100% 160/160 [00:49<00:00,  3.23it/s, training_loss=0.00323, train_accuracy=1, val_loss=1.6, val_accuracy=0.62]\n",
      "Epoch 41: 100% 160/160 [00:48<00:00,  3.28it/s, training_loss=0.00329, train_accuracy=1, val_loss=1.62, val_accuracy=0.616]\n",
      "Epoch 42: 100% 160/160 [00:50<00:00,  3.16it/s, training_loss=0.00342, train_accuracy=1, val_loss=1.63, val_accuracy=0.618]\n",
      "Epoch 43: 100% 160/160 [00:51<00:00,  3.13it/s, training_loss=0.00346, train_accuracy=1, val_loss=1.61, val_accuracy=0.613]\n",
      "Epoch 44: 100% 160/160 [00:52<00:00,  3.02it/s, training_loss=0.00344, train_accuracy=1, val_loss=1.65, val_accuracy=0.611]\n",
      "Epoch 45: 100% 160/160 [00:50<00:00,  3.19it/s, training_loss=0.00347, train_accuracy=1, val_loss=1.67, val_accuracy=0.606]\n",
      "Epoch 46: 100% 160/160 [00:50<00:00,  3.17it/s, training_loss=0.00347, train_accuracy=1, val_loss=1.62, val_accuracy=0.609]\n",
      "Epoch 47: 100% 160/160 [00:48<00:00,  3.30it/s, training_loss=0.00339, train_accuracy=1, val_loss=1.68, val_accuracy=0.598]\n",
      "Epoch 48: 100% 160/160 [00:50<00:00,  3.19it/s, training_loss=0.00331, train_accuracy=1, val_loss=1.67, val_accuracy=0.597]\n",
      "Epoch 49: 100% 160/160 [00:49<00:00,  3.26it/s, training_loss=0.00338, train_accuracy=1, val_loss=2.33, val_accuracy=0.513]\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "history = fit(model, trainloader, optimizer, loss_function, lr_scheduler, epochs=50, valid_loader = validloader, measure = ['accuracy'], verbose = 1)\n",
    "#history = fit(model, smallloader, optimizer, loss_function, lr_scheduler, epochs=40, measure = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#plot learning curve for loss and accuracy\n",
    "file_name = \"norm_zca_relu_model\"\n",
    "epochs = np.arange(50)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.suptitle(\"Normalization ZCA relu Model\")\n",
    "ax[0].plot(epochs, history['training history'], linestyle='-', color='b', label='training')\n",
    "ax[0].plot(epochs, history['validation history'], linestyle='-', color='r', label='validation')\n",
    "ax[0].set_xlabel(\"epoch\")\n",
    "ax[0].set_ylabel(\"loss\")\n",
    "\n",
    "ax[1].plot(epochs, history['training accuracy'], linestyle='-', color='b', label='training')\n",
    "ax[1].plot(epochs, history['validation accuracy'], linestyle='-', color='r', label='validation')\n",
    "ax[1].set_xlabel(\"epoch\")\n",
    "ax[1].set_ylabel(\"accuracy\")\n",
    "\n",
    "ax[1].legend(loc='lower right')\n",
    "fig.savefig(file_name+\".png\", format='png', bbox_inches = 'tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwang75/tmp/workspace/models/base_model.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "test_predict = predict(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5043"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = np.concatenate([yb for _, yb in testloader])\n",
    "categorical_accuracy(np.argmax(test_predict, axis=1).flatten(), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
