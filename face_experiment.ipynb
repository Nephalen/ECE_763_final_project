{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load library\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "#load models and other customized component\n",
    "from models.base_model import BaseModel, MagicModel1, MagicModel2\n",
    "from utils.model_util import fit, predict, hyp_random_search\n",
    "from utils.loss_functions import ProbCatCrossEntropyLoss\n",
    "from utils.performance import categorical_accuracy\n",
    "from utils.preprocessing import GlobalContrastNormalization, ZCATransformation, zca_whitening_matrix\n",
    "from utils.data import FaceDataset\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FaceDataset('data/face_data/', training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set norm zca transform\n",
    "#calculate ZCA whitening\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.ToPILImage(),\n",
    "     transforms.Resize(32),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data (_, 1, 16, 16), resize to (_, 1, 32, 32)\n",
    "trainset = FaceDataset('data/face_data/', training=True, transform=transform)\n",
    "testset = FaceDataset('data/face_data/', training=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split validation set from training set\n",
    "smallset = Subset(trainset, np.arange(200))\n",
    "\n",
    "train_length = len(trainset)\n",
    "train_index = np.arange(int(train_length*0.8))\n",
    "valid_index = np.arange(int(train_length*0.8), train_length)\n",
    "\n",
    "validset = Subset(trainset, valid_index)\n",
    "trainset = Subset(trainset, train_index)\n",
    "\n",
    "smallloader = DataLoader(smallset, batch_size = 4, shuffle=False, num_workers=4, pin_memory =True)\n",
    "trainloader = DataLoader(trainset, batch_size=250, shuffle=True, num_workers=4, pin_memory =True)\n",
    "validloader = DataLoader(validset, batch_size=250, shuffle=False, num_workers=4, pin_memory =True)\n",
    "testloader = DataLoader(testset, batch_size=250, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model, optimizer, etc\n",
    "model = MagicModel1((1, 32, 32), 10)\n",
    "loss_function = ProbCatCrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3) #, momentum=0.9\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, 10, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4% 1/24 [00:00<00:13,  1.65it/s, training_loss=2.3, train_accuracy=0.016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwang75/tmp/workspace/models/base_model.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100% 24/24 [00:04<00:00,  5.32it/s, training_loss=0.329, train_accuracy=0.896, val_loss=0.159, val_accuracy=0.972]\n",
      "Epoch 1: 100% 24/24 [00:04<00:00,  5.66it/s, training_loss=0.0274, train_accuracy=0.99, val_loss=0.0243, val_accuracy=0.992]\n",
      "Epoch 2: 100% 24/24 [00:04<00:00,  5.69it/s, training_loss=0.0153, train_accuracy=0.995, val_loss=0.0123, val_accuracy=0.997]\n",
      "Epoch 3: 100% 24/24 [00:04<00:00,  5.67it/s, training_loss=0.00819, train_accuracy=0.998, val_loss=0.00834, val_accuracy=0.997]\n",
      "Epoch 4: 100% 24/24 [00:04<00:00,  5.63it/s, training_loss=0.00338, train_accuracy=1, val_loss=0.00806, val_accuracy=0.996]\n",
      "Epoch 5: 100% 24/24 [00:04<00:00,  5.65it/s, training_loss=0.00223, train_accuracy=1, val_loss=0.00495, val_accuracy=0.996]\n",
      "Epoch 6: 100% 24/24 [00:04<00:00,  5.64it/s, training_loss=0.00129, train_accuracy=1, val_loss=0.00892, val_accuracy=0.997]\n",
      "Epoch 7: 100% 24/24 [00:04<00:00,  5.64it/s, training_loss=0.00118, train_accuracy=1, val_loss=0.00761, val_accuracy=0.996]\n",
      "Epoch 8: 100% 24/24 [00:04<00:00,  5.65it/s, training_loss=0.000973, train_accuracy=1, val_loss=0.00541, val_accuracy=0.997]\n",
      "Epoch 9: 100% 24/24 [00:04<00:00,  5.70it/s, training_loss=0.00088, train_accuracy=1, val_loss=0.00727, val_accuracy=0.996]\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "history = fit(model, trainloader, optimizer, loss_function, lr_scheduler, epochs=10, valid_loader = validloader, measure = ['accuracy'], verbose = 1)\n",
    "#history = fit(model, smallloader, optimizer, loss_function, lr_scheduler, epochs=40, measure = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#plot learning curve for loss and accuracy\n",
    "file_name = \"norm_zca_relu_model\"\n",
    "epochs = np.arange(50)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.suptitle(\"Normalization ZCA relu Model\")\n",
    "ax[0].plot(epochs, history['training history'], linestyle='-', color='b', label='training')\n",
    "ax[0].plot(epochs, history['validation history'], linestyle='-', color='r', label='validation')\n",
    "ax[0].set_xlabel(\"epoch\")\n",
    "ax[0].set_ylabel(\"loss\")\n",
    "\n",
    "ax[1].plot(epochs, history['training accuracy'], linestyle='-', color='b', label='training')\n",
    "ax[1].plot(epochs, history['validation accuracy'], linestyle='-', color='r', label='validation')\n",
    "ax[1].set_xlabel(\"epoch\")\n",
    "ax[1].set_ylabel(\"accuracy\")\n",
    "\n",
    "ax[1].legend(loc='lower right')\n",
    "fig.savefig(file_name+\".png\", format='png', bbox_inches = 'tight', pad_inches=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "test_predict = predict(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9978729061419835"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = np.concatenate([yb for _, yb in testloader])\n",
    "categorical_accuracy(np.argmax(test_predict, axis=1).flatten(), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
